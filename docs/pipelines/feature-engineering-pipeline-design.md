# ç‰¹å¾å·¥ç¨‹æ•°æ®ç®¡é“è®¾è®¡

## ğŸ“‹ æ¦‚è¿°

### è®¾è®¡ç›®æ ‡
æ„å»ºç»Ÿä¸€çš„ç‰¹å¾å·¥ç¨‹ç®¡é“ï¼Œæ”¯æŒå¹¿å‘Šæ•°æ®çš„æ‰¹é‡å¤„ç†å’Œå®æ—¶ç‰¹å¾è®¡ç®—ï¼Œä¸º Ad Minerï¼ˆæ¨èå¼•æ“ï¼‰å’Œ Adset Allocatorï¼ˆé¢„ç®—åˆ†é…ï¼‰æä¾›é«˜è´¨é‡ç‰¹å¾ã€‚

### æ•°æ®ç±»å‹
- **æ—¶é—´åºåˆ—æ•°æ®**: å¹¿å‘Šè¡¨ç°éšæ—¶é—´å˜åŒ–çš„æŒ‡æ ‡
- **ç±»åˆ«æ•°æ®**: å¹¿å‘Šå±æ€§ã€å—ä¼—å®šä½ã€åˆ›æ„ç±»å‹
- **æ•°å€¼æ•°æ®**: é¢„ç®—ã€èŠ±è´¹ã€è½¬åŒ–ç­‰æŒ‡æ ‡
- **å›¾åƒæ•°æ®**: åˆ›æ„å›¾ç‰‡ï¼ˆé€šè¿‡ GPT-4 Vision æå–ç‰¹å¾ï¼‰

### å®æ–½é‡Œç¨‹ç¢‘

```
Phase 1: Python è„šæœ¬ (MVP)
    â†“
Phase 2: Spark æ‰¹å¤„ç† (Production)
    â†“
Phase 3: æµå¤„ç† (Real-time)
```

---

## ğŸ—ºï¸ å®æ–½é‡Œç¨‹ç¢‘

### Phase 1: Python è„šæœ¬ MVP

**ç›®æ ‡**: å¿«é€ŸéªŒè¯ç‰¹å¾å·¥ç¨‹æ–¹æ³•ï¼Œå»ºç«‹åŸºçº¿

**æŠ€æœ¯æ ˆ**:
- Python 3.10+
- Pandas / NumPy
- Scikit-learn
- Jupyter Lab (å¼€å‘)
- Python è„šæœ¬ (ç”Ÿäº§)

**äº¤ä»˜ç‰©**:
```python
# scripts/feature_pipeline.py
python scripts/feature_pipeline.py \
  --customer customer_123 \
  --input data/raw/ad_data.csv \
  --output data/features/ \
  --features all
```

**ç‰¹å¾åˆ—è¡¨**: 50-100 ä¸ªæ ¸å¿ƒç‰¹å¾

**å¤„ç†èƒ½åŠ›**:
- æ•°æ®è§„æ¨¡: < 10GB
- å¤„ç†æ—¶é—´: < 1 hour
- è°ƒåº¦: Cron æ¯æ—¥è¿è¡Œ

**éªŒæ”¶æ ‡å‡†**:
- [ ] èƒ½å¤Ÿå¤„ç† 30 å¤©å†å²æ•°æ®
- [ ] ç”Ÿæˆ 50+ ç‰¹å¾
- [ ] é€šè¿‡å•å…ƒæµ‹è¯•
- [ ] æ–‡æ¡£å®Œæ•´

---

### Phase 2: Spark æ‰¹å¤„ç†

**ç›®æ ‡**: ç”Ÿäº§çº§æ‰¹å¤„ç†ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®

**æŠ€æœ¯æ ˆ**:
- Apache Spark 3.x (PySpark)
- AWS EMR / Dataproc
- Airflow è°ƒåº¦
- Parquet å­˜å‚¨
- MLflow ç‰¹å¾è·Ÿè¸ª

**äº¤ä»˜ç‰©**:
```python
# jobs/spark_feature_pipeline.py
spark-submit jobs/spark_feature_pipeline.py \
  --customer customer_123 \
  --input s3://data-bucket/raw/ \
  --output s3://features-bucket/ \
  --date 2025-01-29
```

**ç‰¹å¾åˆ—è¡¨**: 200-300 ä¸ªç‰¹å¾

**å¤„ç†èƒ½åŠ›**:
- æ•°æ®è§„æ¨¡: 100GB - 1TB
- å¤„ç†æ—¶é—´: < 30 min
- è°ƒåº¦: Airflow DAG

**éªŒæ”¶æ ‡å‡†**:
- [ ] å¤„ç† 1TB æ•°æ® < 30 min
- [ ] ç”Ÿæˆ 200+ ç‰¹å¾
- [ ] é›†æˆ Airflow
- [ ] ç‰¹å¾ç‰ˆæœ¬ç®¡ç† (MLflow)

---

### Phase 3: æµå¤„ç†

**ç›®æ ‡**: å®æ—¶ç‰¹å¾è®¡ç®—ï¼Œæ”¯æŒåœ¨çº¿å†³ç­–

**æŠ€æœ¯æ ˆ**:
- AWS Lambda / Step Functions
- Amazon Kinesis
- Redis (åœ¨çº¿ç‰¹å¾å­˜å‚¨)
- FastAPI (ç‰¹å¾æœåŠ¡)
- Kafka (å¯é€‰)

**äº¤ä»˜ç‰©**:
```python
# Real-time Feature Service
curl -X POST https://features.api.example.com/update \
  -H "Content-Type: application/json" \
  -d '{"ad_id": "123", "event_type": "metrics_update", ...}'
```

**ç‰¹å¾åˆ—è¡¨**: 300+ ç‰¹å¾ï¼ˆåŒ…æ‹¬å®æ—¶ç‰¹å¾ï¼‰

**å¤„ç†èƒ½åŠ›**:
- å»¶è¿Ÿ: < 1 second
- åå: 1000+ events/sec
- å¯ç”¨æ€§: 99.9%

**éªŒæ”¶æ ‡å‡†**:
- [ ] ç«¯åˆ°ç«¯å»¶è¿Ÿ < 1 sec
- [ ] æ”¯æŒ 1000+ QPS
- [ ] ç‰¹å¾æ–°é²œåº¦ < 5 sec
- [ ] å®Œæ•´ç›‘æ§å’Œå‘Šè­¦

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®æºå±‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Meta Ads API â”‚  â”‚ Historical   â”‚  â”‚ Webhook      â”‚     â”‚
â”‚  â”‚ (Real-time)  â”‚  â”‚ Exports      â”‚  â”‚ Events       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®æ¥å…¥å±‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Batch        â”‚  â”‚ Stream       â”‚  â”‚ Image        â”‚     â”‚
â”‚  â”‚ Ingestion    â”‚  â”‚ Ingestion    â”‚  â”‚ Processing   â”‚     â”‚
â”‚  â”‚ Phase 1/2    â”‚  â”‚ Phase 3      â”‚  â”‚ (GPT-4V)     â”‚     â”‚
â”‚  â”‚ (Python/     â”‚  â”‚ (Kinesis)    â”‚  â”‚              â”‚     â”‚
â”‚  â”‚  Spark)      â”‚  â”‚              â”‚  â”‚              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                   â”‚
        â†“                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ‰¹é‡ç‰¹å¾ç®¡é“          â”‚       â”‚  å®æ—¶ç‰¹å¾ç®¡é“          â”‚
â”‚  Phase 1/2            â”‚       â”‚  Phase 3              â”‚
â”‚                       â”‚       â”‚                       â”‚
â”‚  - Python (MVP)       â”‚       â”‚  - Lambda/Step Func   â”‚
â”‚  - Spark (Prod)       â”‚       â”‚  - Incremental        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç‰¹å¾è®¡ç®—å¼•æ“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  æ•°å€¼ç‰¹å¾å¼•æ“     â”‚  ç±»åˆ«ç‰¹å¾å¼•æ“   â”‚  æ—¶åºç‰¹å¾å¼•æ“  â”‚  â”‚
â”‚  â”‚  - 200+ æ•°å€¼æŠ€æœ¯ â”‚  - 100+ ç±»åˆ«æŠ€æœ¯â”‚  - 150+ æ—¶åºæŠ€æœ¯â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              äº¤äº’ç‰¹å¾å¼•æ“ (500+ ç»„åˆ)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           ç‰¹å¾åç§°æ··æ·† (f1, f2, ...)                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç‰¹å¾åç§°æ˜ å°„å±‚ (Privacy Layer)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  f1: impressions_mean                              â”‚  â”‚
â”‚  â”‚  f2: ctr                                           â”‚  â”‚
â”‚  â”‚  f3: objective_format_combo                        â”‚  â”‚
â”‚  â”‚  ...                                               â”‚  â”‚
â”‚  â”‚  Total: 500+ features â†’ f1...f500                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç‰¹å¾å­˜å‚¨å±‚ (Feature Store)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Online Store  â”‚  â”‚ Offline Store â”‚  â”‚ Metadata     â”‚     â”‚
â”‚  â”‚ (Redis/Dynamo)â”‚  â”‚ (S3/Parquet)  â”‚  â”‚ (MLflow)     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ¶ˆè´¹è€…å±‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Ad Miner     â”‚  â”‚ Adset        â”‚  â”‚ Analytics    â”‚     â”‚
â”‚  â”‚ (æ¨èå¼•æ“)    â”‚  â”‚ Allocator    â”‚  â”‚ Dashboard    â”‚     â”‚
â”‚  â”‚              â”‚  â”‚ (é¢„ç®—åˆ†é…)    â”‚  â”‚              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Part 1: ç‰¹å¾åç§°æ··æ·†ï¼ˆéšç§ä¿æŠ¤ï¼‰

### 1.1 è®¾è®¡åŸåˆ™

**ç›®æ ‡**: é€šè¿‡ç‰¹å¾åç§°æ··æ·†ä¿æŠ¤å®¢æˆ·æ•°æ®éšç§ï¼ŒåŒæ—¶ä¿æŒå†…éƒ¨å¯è¿½æº¯æ€§ã€‚

```python
# æ˜ å°„ç¤ºä¾‹
{
    # å†…éƒ¨åç§° â†’ æ··æ·†åç§°
    "impressions_mean": "f1",
    "clicks_sum": "f2",
    "ctr": "f3",
    "roas_7d_avg": "f4",
    "objective_format_combo": "f5",
    "campaign_objective_encoding": "f6",
    ...
}

# å¯¹å®¢æˆ·/å·¥ç¨‹å¸ˆæš´éœ²çš„ç‰¹å¾å‘é‡
{
    "entity_id": "ad_123",
    "features": {
        "f1": 15000.5,
        "f2": 500,
        "f3": 3.33,
        "f4": 2.5,
        "f5": 0,
        "f6": 1,
        ...
    }
}
```

### 1.2 æ˜ å°„ç³»ç»Ÿå®ç°

```python
# src/pipelines/privacy/feature_name_obfuscator.py

import hashlib
import json
from typing import Dict, List, Optional
from pathlib import Path

class FeatureNameObfuscator:
    """
    ç‰¹å¾åç§°æ··æ·†å™¨

    åŸåˆ™:
    1. ç¡®å®šæ€§æ˜ å°„ï¼ˆåŒä¸€å â†’ åŒä¸€ fXï¼‰
    2. ä¸å¯é€†ï¼ˆfX æ— æ³•åæ¨å‡ºåŸå§‹åç§°ï¼‰
    3. å¯è¿½æº¯ï¼ˆå†…éƒ¨ç»´æŠ¤æ˜ å°„è¡¨ï¼‰
    4. ç‰ˆæœ¬æ§åˆ¶ï¼ˆæ˜ å°„ç‰ˆæœ¬åŒ–ï¼‰
    """

    def __init__(self, version: str = "1.0"):
        self.version = version
        self.mapping = self._load_mapping()

    def obfuscate(self, feature_name: str) -> str:
        """
        å°†ç‰¹å¾åç§°æ··æ·†ä¸º f1, f2, ...

        ä½¿ç”¨å“ˆå¸Œç¡®ä¿ç¡®å®šæ€§æ˜ å°„
        """
        # è®¡ç®—å“ˆå¸Œ
        hash_value = int(hashlib.sha256(
            f"{feature_name}_{self.version}".encode()
        ).hexdigest(), 16)

        # è½¬æ¢ä¸º f1-f999
        feature_index = (hash_value % 999) + 1

        return f"f{feature_index}"

    def obfuscate_dict(self, features: Dict[str, any]) -> Dict[str, any]:
        """æ‰¹é‡æ··æ·†ç‰¹å¾å­—å…¸"""
        obfuscated = {}
        mapping_record = {}

        for name, value in features.items():
            f_name = self.obfuscate(name)
            obfuscated[f_name] = value
            mapping_record[f_name] = name

        # ä¿å­˜æ˜ å°„ï¼ˆä»…å†…éƒ¨è®¿é—®ï¼‰
        self._save_mapping_record(mapping_record)

        return obfuscated

    def deobfuscate(self, f_name: str) -> Optional[str]:
        """åæ··æ·†ï¼ˆä»…å†…éƒ¨ä½¿ç”¨ï¼‰"""
        return self.mapping.get(f_name)

    def _load_mapping(self) -> Dict[str, str]:
        """åŠ è½½æ˜ å°„è¡¨ï¼ˆä»å®‰å…¨å­˜å‚¨ï¼‰"""
        mapping_file = Path(f"config/feature_mappings/v{self.version}.json")

        if mapping_file.exists():
            with open(mapping_file) as f:
                return json.load(f)
        else:
            return {}

    def _save_mapping_record(self, record: Dict[str, str]):
        """ä¿å­˜æ˜ å°„è®°å½•ï¼ˆåˆ°å®‰å…¨å­˜å‚¨ï¼‰"""
        mapping_file = Path("config/feature_mappings/internal.json")

        existing = {}
        if mapping_file.exists():
            with open(mapping_file) as f:
                existing = json.load(f)

        existing.update(record)

        with open(mapping_file, 'w') as f:
            json.dump(existing, f, indent=2)
```

### 1.3 æ˜ å°„è¡¨ç®¡ç†

```python
# config/feature_mappings/v1.0.json (ä»…å†…éƒ¨å¯è®¿é—®)
{
  "f1": "impressions_mean",
  "f2": "clicks_sum",
  "f3": "ctr",
  "f4": "roas_7d_avg",
  "f5": "objective_format_combo",
  "f6": "campaign_objective_encoding",
  "f7": "spend_rolling_std_7d",
  "f8": "impressions_lag_7d",
  ...
}

# å¯¹å¤–æš´éœ²çš„ç‰¹å¾åˆ—è¡¨ (å…¬å¼€)
# config/feature_mappings/public_features.json
{
  "total_features": 500,
  "feature_list": ["f1", "f2", "f3", ..., "f500"],
  "feature_categories": {
    "numerical": ["f1", "f2", ..., "f250"],
    "categorical": ["f251", "f252", ..., "f350"],
    "timeseries": ["f351", "f352", ..., "f500"]
  }
}
```

### 1.4 ä½¿ç”¨ç¤ºä¾‹

```python
# ä½¿ç”¨æ··æ·†åçš„ç‰¹å¾
obfuscator = FeatureNameObfuscator(version="1.0")

# åŸå§‹ç‰¹å¾
raw_features = {
    "impressions_mean": 15000.5,
    "clicks_sum": 500,
    "ctr": 3.33,
    "roas_7d_avg": 2.5
}

# æ··æ·†åï¼ˆå¯ä»¥å®‰å…¨å‘é€ç»™å®¢æˆ·ï¼‰
obfuscated_features = obfuscator.obfuscate_dict(raw_features)
# {
#     "f1": 15000.5,
#     "f2": 500,
#     "f3": 3.33,
#     "f4": 2.5
# }

# å†…éƒ¨ä½¿ç”¨æ—¶å¯ä»¥åæ··æ·†
original_name = obfuscator.deobfuscate("f1")  # "impressions_mean"
```

---

## ğŸ”¢ Part 2: å…¨é¢çš„æ•°å€¼ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ï¼ˆ200+ ç‰¹å¾ï¼‰

### 2.1 åŸºç¡€ç»Ÿè®¡ç‰¹å¾ï¼ˆ50+ï¼‰

```python
class ComprehensiveNumericalFeatures:
    """å…¨é¢çš„æ•°å€¼ç‰¹å¾æå–"""

    def extract_basic_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åŸºç¡€ç»Ÿè®¡ç‰¹å¾ (50+)

        åŒ…æ‹¬:
        - ä¸­å¿ƒè¶‹åŠ¿: å‡å€¼ã€ä¸­ä½æ•°ã€ä¼—æ•°ã€å‡ ä½•å‡å€¼ã€è°ƒå’Œå‡å€¼
        - ç¦»æ•£ç¨‹åº¦: æ ‡å‡†å·®ã€æ–¹å·®ã€èŒƒå›´ã€IQRã€MAD
        - åˆ†ä½æ•°: p1, p5, p10, p25, p50, p75, p90, p95, p99
        - åˆ†å¸ƒå½¢çŠ¶: ååº¦ã€å³°åº¦ã€Jarque-Bera æ£€éªŒ
        - å¼‚å¸¸å€¼: å¼‚å¸¸å€¼æ•°é‡ã€æ¯”ä¾‹ã€Z-score
        - å˜æ¢: Log, Box-Cox, Yeo-Johnson, Quantile
        """

        features = pd.DataFrame(index=df.index)
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            # 1. ä¸­å¿ƒè¶‹åŠ¿ (5)
            features[f'{col}_mean'] = df[col].mean()
            features[f'{col}_median'] = df[col].median()
            features[f'{col}_mode'] = df[col].mode()[0] if not df[col].mode().empty else 0
            features[f'{col}_geometric_mean'] = self._geometric_mean(df[col])
            features[f'{col}_harmonic_mean'] = self._harmonic_mean(df[col])

            # 2. ç¦»æ•£ç¨‹åº¦ (8)
            features[f'{col}_std'] = df[col].std()
            features[f'{col}_var'] = df[col].var()
            features[f'{col}_range'] = df[col].max() - df[col].min()
            features[f'{col}_iqr'] = df[col].quantile(0.75) - df[col].quantile(0.25)
            features[f'{col}_mad'] = df[col].mad()  # Mean Absolute Deviation
            features[f'{col}_cv'] = df[col].std() / (df[col].mean() + 1e-6)  # Coefficient of Variation
            features[f'{col}_range_coefficient'] = (df[col].max() - df[col].min()) / (df[col].mean() + 1e-6)
            features[f'{col}_quartile_coefficient'] = features[f'{col}_iqr'] / (df[col].quantile(0.75) + df[col].quantile(0.25) + 1e-6)

            # 3. åˆ†ä½æ•° (9)
            for q in [1, 5, 10, 25, 50, 75, 90, 95, 99]:
                features[f'{col}_p{q}'] = df[col].quantile(q/100)

            # 4. åˆ†å¸ƒå½¢çŠ¶ (5)
            features[f'{col}_skew'] = df[col].skew()
            features[f'{col}_kurtosis'] = df[col].kurtosis()
            features[f'{col}_jarque_bera'] = self._jarque_bera(df[col])
            features[f'{col}_excess_kurtosis'] = df[col].kurtosis()  # è¶…é¢å³°åº¦
            features[f'{col}_moment_5'] = ((df[col] - df[col].mean()) / df[col].std())**5  # 5é˜¶çŸ©

            # 5. å¼‚å¸¸å€¼ (4)
            Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR)))
            features[f'{col}_outlier_count'] = outliers.sum()
            features[f'{col}_outlier_ratio'] = outliers.mean()
            features[f'{col}_outlier_mean_zscore'] = np.abs((df[col] - df[col].mean()) / df[col].std()).mean()
            features[f'{col}_extreme_outlier_ratio'] = ((np.abs((df[col] - df[col].mean()) / df[col].std()) > 3).mean())

            # 6. æ•°æ®å˜æ¢ (6)
            features[f'{col}_log'] = np.log1p(df[col])
            features[f'{col}_log2'] = np.log2(df[col] + 1)
            features[f'{col}_log10'] = np.log10(df[col] + 1)
            features[f'{col}_sqrt'] = np.sqrt(df[col].abs())
            features[f'{col}_boxcox'], _ = self._boxcox_transform(df[col])
            features[f'{col}_yeojohnson'], _ = self._yeojohnson_transform(df[col])

            # 7. å½’ä¸€åŒ– (4)
            min_val, max_val = df[col].min(), df[col].max()
            features[f'{col}_minmax'] = (df[col] - min_val) / (max_val - min_val + 1e-6)
            features[f'{col}_robust'] = (df[col] - df[col].median()) / (features[f'{col}_iqr'] + 1e-6)
            features[f'{col}_zscore'] = (df[col] - df[col].mean()) / df[col].std()
            features[f'{col}_unit_vector'] = df[col] / (np.linalg.norm(df[col]) + 1e-6)

            # 8. ç™¾åˆ†ä½æ’å (2)
            features[f'{col}_percentile_rank'] = df[col].rank(pct=True)
            features[f'{col}_decile_rank'] = pd.cut(df[col].rank(pct=True), bins=10, labels=False)

        return features

    def _geometric_mean(self, series: pd.Series) -> float:
        """å‡ ä½•å‡å€¼"""
        return np.exp(np.log(series[series > 0]).mean()) if (series > 0).any() else 0

    def _harmonic_mean(self, series: pd.Series) -> float:
        """è°ƒå’Œå‡å€¼"""
        return len(series) / np.sum(1.0 / (series + 1e-6))

    def _jarque_bera(self, series: pd.Series) -> float:
        """Jarque-Bera æ­£æ€æ€§æ£€éªŒ"""
        from scipy.stats import jarque_bera
        return jarque_bera(series.dropna())[0]

    def _boxcox_transform(self, series: pd.Series):
        """Box-Cox å˜æ¢"""
        from scipy.stats import boxcox
        try:
            transformed, _ = boxcox(series + 1 - series.min())
            return transformed.mean(), _
        except:
            return series.mean(), 0

    def _yeojohnson_transform(self, series: pd.Series):
        """Yeo-Johnson å˜æ¢"""
        from sklearn.preprocessing import PowerTransformer
        try:
            pt = PowerTransformer(method='yeo-johnson')
            transformed = pt.fit_transform(series.values.reshape(-1, 1))
            return transformed.mean(), pt.lambdas_[0]
        except:
            return series.mean(), 0
```

### 2.2 é«˜çº§ç»Ÿè®¡ç‰¹å¾ï¼ˆ40+ï¼‰

```python
    def extract_advanced_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é«˜çº§ç»Ÿè®¡ç‰¹å¾ (40+)

        åŒ…æ‹¬:
        - ç†µå’Œäº’ä¿¡æ¯
        - ç›¸å…³ç³»æ•°
        - ç´¯ç§¯ç»Ÿè®¡
        - ç™¾åˆ†ä½å˜åŒ–
        - ç›¸å¯¹å·®å¼‚
        - å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ (ç”¨äºç±»åˆ«ç¼–ç åçš„æ•°å€¼)
        """

        features = pd.DataFrame(index=df.index)
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            # 1. ç†µç‰¹å¾ (3)
            features[f'{col}_entropy'] = self._calculate_entropy(df[col])
            features[f'{col}_conditional_entropy'] = self._conditional_entropy(df[col], df.get('roas', pd.Series()))
            features[f'{col}_mutual_info'] = self._mutual_info(df[col], df.get('conversions', pd.Series()))

            # 2. ç›¸å…³æ€§ (3)
            if 'roas' in df.columns:
                features[f'{col}_correlation_with_roas'] = df[col].corr(df['roas'])
            if 'spend' in df.columns:
                features[f'{col}_correlation_with_spend'] = df[col].corr(df['spend'])
            if 'impressions' in df.columns:
                features[f'{col}_correlation_with_impressions'] = df[col].corr(df['impressions'])

            # 3. ç´¯ç§¯ç»Ÿè®¡ (5)
            features[f'{col}_cumsum'] = df[col].cumsum()
            features[f'{col}_cummax'] = df[col].cummax()
            features[f'{col}_cummin'] = df[col].cummin()
            features[f'{col}_cummean'] = df[col].expanding().mean()
            features[f'{col}_cumstd'] = df[col].expanding().std()

            # 4. ç™¾åˆ†ä½å˜åŒ– (4)
            features[f'{col}_pct_change_1'] = df[col].pct_change(1)
            features[f'{col}_pct_change_7'] = df[col].pct_change(7)
            features[f'{col}_pct_change_30'] = df[col].pct_change(30)
            features[f'{col}_pct_change_90'] = df[col].pct_change(90)

            # 5. ç›¸å¯¹å·®å¼‚ (4)
            features[f'{col}_diff_from_mean'] = df[col] - df[col].mean()
            features[f'{col}_diff_from_median'] = df[col] - df[col].median()
            features[f'{col}_pct_diff_from_mean'] = ((df[col] - df[col].mean()) / (df[col].mean() + 1e-6)) * 100
            features[f'{col}_pct_diff_from_median'] = ((df[col] - df[col].median()) / (df[col].median() + 1e-6)) * 100

            # 6. åŠ æƒç»Ÿè®¡ (3)
            weights = df.get('impressions', pd.Series([1]*len(df)))
            features[f'{col}_weighted_mean'] = np.average(df[col], weights=weights)
            features[f'{col}_weighted_std'] = np.sqrt(np.average((df[col] - features[f'{col}_weighted_mean'])**2, weights=weights))
            features[f'{col}_weighted_sum'] = (df[col] * weights).sum()

            # 7. ç¼©æ”¾ç»Ÿè®¡ (3)
            features[f'{col}_sum_squares'] = (df[col] ** 2).sum()
            features[f'{col}_norm_l1'] = np.abs(df[col]).sum()
            features[f'{col}_norm_l2'] = np.sqrt((df[col] ** 2).sum())

        return features

    def _calculate_entropy(self, series: pd.Series, n_bins: int = 10) -> float:
        """è®¡ç®—ç†µ"""
        counts, _ = np.histogram(series.dropna(), bins=n_bins)
        probabilities = counts / counts.sum()
        probabilities = probabilities[probabilities > 0]
        return -np.sum(probabilities * np.log2(probabilities))

    def _conditional_entropy(self, x: pd.Series, y: pd.Series) -> float:
        """æ¡ä»¶ç†µ"""
        if y.empty:
            return 0
        # ç®€åŒ–å®ç°
        return self._calculate_entropy(x) * 0.8

    def _mutual_info(self, x: pd.Series, y: pd.Series) -> float:
        """äº’ä¿¡æ¯"""
        from sklearn.metrics import mutual_info_score
        if y.empty:
            return 0
        # ç¦»æ•£åŒ–
        x_discrete = pd.cut(x, bins=10, labels=False)
        y_discrete = pd.cut(y, bins=10, labels=False)
        return mutual_info_score(x_discrete, y_discrete)
```

### 2.3 æ—¶é—´åºåˆ—æ•°å€¼ç‰¹å¾ï¼ˆ60+ï¼‰

```python
    def extract_timeseries_numerical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ—¶é—´åºåˆ—æ•°å€¼ç‰¹å¾ (60+)

        åŒ…æ‹¬:
        - æ»šåŠ¨çª—å£ç»Ÿè®¡ (å„ç§çª—å£å¤§å°)
        - æ»åç‰¹å¾
        - å·®åˆ†ç‰¹å¾
        - ç™¾åˆ†æ¯”å˜åŒ–
        - åŠ é€Ÿåº¦
        - åŠ¨é‡
        - è¶‹åŠ¿
        - æ³¢åŠ¨ç‡
        - è‡ªç›¸å…³
        """

        features = pd.DataFrame(index=df.index)
        numeric_cols = ['impressions', 'clicks', 'spend', 'conversions', 'roas']

        for col in numeric_cols:
            if col not in df.columns:
                continue

            # 1. æ»šåŠ¨çª—å£ - ä¸­å¿ƒè¶‹åŠ¿ (12)
            for window in [3, 7, 14, 30]:
                features[f'{col}_rolling_mean_{window}d'] = df[col].rolling(window).mean()
                features[f'{col}_rolling_median_{window}d'] = df[col].rolling(window).median()
                features[f'{col}_rolling_expmean_{window}d'] = df[col].ewm(span=window).mean()

            # 2. æ»šåŠ¨çª—å£ - ç¦»æ•£ç¨‹åº¦ (12)
            for window in [3, 7, 14, 30]:
                features[f'{col}_rolling_std_{window}d'] = df[col].rolling(window).std()
                features[f'{col}_rolling_var_{window}d'] = df[col].rolling(window).var()
                features[f'{col}_rolling_range_{window}d'] = df[col].rolling(window).max() - df[col].rolling(window).min()

            # 3. æ»šåŠ¨çª—å£ - ç´¯ç§¯ (8)
            for window in [7, 14, 30]:
                features[f'{col}_rolling_sum_{window}d'] = df[col].rolling(window).sum()
                features[f'{col}_rolling_min_{window}d'] = df[col].rolling(window).min()
                features[f'{col}_rolling_max_{window}d'] = df[col].rolling(window).max()

            # 4. æ»åç‰¹å¾ (10)
            for lag in [1, 2, 3, 7, 14, 30, 60, 90]:
                features[f'{col}_lag_{lag}d'] = df[col].shift(lag)

            # 5. å·®åˆ†ç‰¹å¾ (5)
            features[f'{col}_diff_1'] = df[col].diff(1)
            features[f'{col}_diff_7'] = df[col].diff(7)
            features[f'{col}_diff_30'] = df[col].diff(30)
            features[f'{col}_diff_pct_1'] = df[col].pct_change(1)
            features[f'{col}_diff_pct_7'] = df[col].pct_change(7)

            # 6. åŠ é€Ÿåº¦å’ŒåŠ¨é‡ (4)
            features[f'{col}_acceleration'] = df[col].diff(1).diff(1)
            features[f'{col}_momentum_7d'] = df[col] - df[col].shift(7)
            features[f'{col}_momentum_30d'] = df[col] - df[col].shift(30)
            features[f'{col}_roc_7d'] = ((df[col] - df[col].shift(7)) / df[col].shift(7)) * 100  # Rate of Change

            # 7. æ³¢åŠ¨ç‡ (4)
            for window in [7, 14, 30]:
                features[f'{col}_volatility_{window}d'] = df[col].pct_change().rolling(window).std()
                features[f'{col}_volatility_exp_{window}d'] = df[col].pct_change().ewm(span=window).std()

            # 8. è‡ªç›¸å…³ (3)
            for lag in [1, 7, 14]:
                features[f'{col}_autocorr_lag{lag}'] = df[col].autocorr(lag=lag)

            # 9. è¶‹åŠ¿ç‰¹å¾ (3)
            features[f'{col}_trend_slope_7d'] = df[col].rolling(7).apply(self._linear_slope)
            features[f'{col}_trend_r2_7d'] = df[col].rolling(7).apply(self._trend_r2)
            features[f'{col}_trend_strength'] = df[col].rolling(30).apply(lambda x: np.corrcoef(range(len(x)), x)[0, 1]**2 if len(x) > 1 else 0)

        return features

    def _linear_slope(self, series):
        """è®¡ç®—çº¿æ€§è¶‹åŠ¿æ–œç‡"""
        if len(series) < 2:
            return 0
        x = np.arange(len(series))
        return np.polyfit(x, series, 1)[0]

    def _trend_r2(self, series):
        """è®¡ç®—è¶‹åŠ¿çš„ RÂ²"""
        if len(series) < 2:
            return 0
        x = np.arange(len(series))
        slope, intercept = np.polyfit(x, series, 1)
        y_pred = slope * x + intercept
        ss_res = np.sum((series - y_pred) ** 2)
        ss_tot = np.sum((series - series.mean()) ** 2)
        return 1 - (ss_res / (ss_tot + 1e-6))
```

### 2.4 æ¯”ç‡å’Œè¡ç”Ÿç‰¹å¾ï¼ˆ30+ï¼‰

```python
    def extract_ratio_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¯”ç‡å’Œè¡ç”Ÿç‰¹å¾ (30+)

        åŒ…æ‹¬:
        - æ•ˆç‡æŒ‡æ ‡ (CTR, CVR, ROAS, CPA, CPC, CPM)
        - å¤åˆæ¯”ç‡
        - ç›¸å¯¹æ¯”ç‡
        - ç™¾åˆ†ä½æ¯”ç‡
        - å€’æ•°ç‰¹å¾
        """

        features = pd.DataFrame(index=df.index)

        # 1. åŸºç¡€æ•ˆç‡æŒ‡æ ‡ (7)
        features['ctr'] = (df['clicks'] / (df['impressions'] + 1e-6)) * 100
        features['cvr'] = (df['conversions'] / (df['clicks'] + 1e-6)) * 100
        features['roas'] = df['revenue'] / (df['spend'] + 1e-6)
        features['cpa'] = df['spend'] / (df['conversions'] + 1e-6)
        features['cpc'] = df['spend'] / (df['clicks'] + 1e-6)
        features['cpm'] = (df['spend'] / (df['impressions'] + 1e-6)) * 1000
        features['rpm'] = (df['revenue'] / (df['impressions'] + 1e-6)) * 1000  # Revenue per Mille

        # 2. å¤åˆæ¯”ç‡ (8)
        features['roas_per_click'] = df['revenue'] / (df['clicks'] + 1e-6)
        features['revenue_per_impression'] = df['revenue'] / (df['impressions'] + 1e-6)
        features['cost_per_impression'] = df['spend'] / (df['impressions'] + 1e-6)
        features['conversion_value'] = df['revenue'] / (df['conversions'] + 1e-6)
        features['click_to_conversion_ratio'] = df['conversions'] / (df['clicks'] + 1e-6)
        features['spend_to_budget_ratio'] = df['spend'] / (df['budget'] + 1e-6)
        features['impressions_per_click'] = df['impressions'] / (df['clicks'] + 1e-6)
        features['cost_efficiency'] = df['revenue'] / (df['spend'] + 1e-6)  # Same as ROAS

        # 3. ç›¸å¯¹æ¯”ç‡ (5)
        features['ctr_vs_benchmark'] = features['ctr'] / (features['ctr'].mean() + 1e-6)
        features['cvr_vs_benchmark'] = features['cvr'] / (features['cvr'].mean() + 1e-6)
        features['roas_vs_benchmark'] = features['roas'] / (features['roas'].mean() + 1e-6)
        features['cpm_vs_benchmark'] = features['cpm'] / (features['cpm'].mean() + 1e-6)
        features['efficiency_score'] = (features['ctr'] * features['cvr'] * features['roas']) ** (1/3)

        # 4. ç™¾åˆ†ä½æ¯”ç‡ (4)
        for col in ['ctr', 'cvr', 'roas', 'cpm']:
            percentile = df[col].rank(pct=True)
            features[f'{col}_percentile'] = percentile
            features[f'{col}_is_top_10pct'] = (percentile >= 0.9).astype(int)
            features[f'{col}_is_top_25pct'] = (percentile >= 0.75).astype(int)
            features[f'{col}_is_bottom_25pct'] = (percentile <= 0.25).astype(int)

        # 5. å€’æ•°ç‰¹å¾ (3)
        features['impressions_inverse'] = 1 / (df['impressions'] + 1e-6)
        features['spend_inverse'] = 1 / (df['spend'] + 1e-6)
        features['cost_per_unit_inverse'] = 1 / (features['cpa'] + 1e-6)

        return features
```

### 2.5 ç¨³å®šæ€§å’Œå¼‚å¸¸æ£€æµ‹ç‰¹å¾ï¼ˆ20+ï¼‰

```python
    def extract_stability_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç¨³å®šæ€§å’Œå¼‚å¸¸æ£€æµ‹ç‰¹å¾ (20+)

        åŒ…æ‹¬:
        - æ³¢åŠ¨ç‡æŒ‡æ ‡
        - ç¨³å®šæ€§å¾—åˆ†
        - å˜åŒ–ç‚¹æ£€æµ‹
        - å¼‚å¸¸åˆ†æ•°
        """

        features = pd.DataFrame(index=df.index)
        numeric_cols = ['impressions', 'clicks', 'spend', 'roas']

        for col in numeric_cols:
            if col not in df.columns:
                continue

            # 1. æ³¢åŠ¨ç‡æŒ‡æ ‡ (8)
            features[f'{col}_volatility_7d'] = df[col].rolling(7).std() / (df[col].rolling(7).mean() + 1e-6)
            features[f'{col}_volatility_14d'] = df[col].rolling(14).std() / (df[col].rolling(14).mean() + 1e-6)
            features[f'{col}_volatility_30d'] = df[col].rolling(30).std() / (df[col].rolling(30).mean() + 1e-6)
            features[f'{col}_max_drawdown'] = df[col].rolling(30).apply(lambda x: (x.max() - x.min()) / x.max())
            features[f'{col}_price_variation'] = (df[col].rolling(30).max() - df[col].rolling(30).min()) / (df[col].rolling(30).mean() + 1e-6)
            features[f'{col}_avg_directional_change'] = np.abs(df[col].diff(1)).mean()
            features[f'{col}_volatility_ratio_7_30'] = features[f'{col}_volatility_7d'] / (features[f'{col}_volatility_30d'] + 1e-6)
            features[f'{col}_stability_index'] = 1 / (1 + features[f'{col}_volatility_30d'])

            # 2. ç¨³å®šæ€§ç‰¹å¾ (4)
            features[f'{col}_consecutive_up'] = (df[col].diff(1) > 0).astype(int).groupby((df[col].diff(1) <= 0).cumsum()).cumsum()
            features[f'{col}_consecutive_down'] = (df[col].diff(1) < 0).astype(int).groupby((df[col].diff(1) >= 0).cumsum()).cumsum()
            features[f'{col}_direction_changes'] = ((df[col].diff(1) > 0) != (df[col].diff(1).shift(1) > 0)).astype(int).cumsum()
            features[f'{col}_stability_score'] = 1 - (features[f'{col}_direction_changes'] / len(df))

            # 3. å¼‚å¸¸æ£€æµ‹ (5)
            rolling_mean = df[col].rolling(30).mean()
            rolling_std = df[col].rolling(30).std()
            features[f'{col}_zscore'] = (df[col] - rolling_mean) / (rolling_std + 1e-6)
            features[f'{col}_is_anomaly_zscore'] = (np.abs(features[f'{col}_zscore']) > 3).astype(int)

            rolling_Q1 = df[col].rolling(30).quantile(0.25)
            rolling_Q3 = df[col].rolling(30).quantile(0.75)
            rolling_IQR = rolling_Q3 - rolling_Q1
            features[f'{col}_iqr_anomaly'] = ((df[col] < (rolling_Q1 - 1.5 * rolling_IQR)) | (df[col] > (rolling_Q3 + 1.5 * rolling_IQR))).astype(int)

            features[f'{col}_anomaly_score'] = np.abs(features[f'{col}_zscore']) * features[f'{col}_iqr_anomaly']

        return features
```

---

## ğŸ·ï¸ Part 3: å…¨é¢çš„ç±»åˆ«ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ï¼ˆ100+ ç‰¹å¾ï¼‰

### 3.1 åŸºç¡€ç¼–ç æŠ€æœ¯ï¼ˆ20+ï¼‰

```python
class ComprehensiveCategoricalFeatures:
    """å…¨é¢çš„ç±»åˆ«ç‰¹å¾æå–"""

    def extract_basic_encoding(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åŸºç¡€ç¼–ç æŠ€æœ¯ (20+)

        åŒ…æ‹¬:
        - Label Encoding
        - One-Hot Encoding
        - Binary Encoding
        - BaseN Encoding
        - Ordinal Encoding
        """

        features = pd.DataFrame(index=df.index)
        categorical_cols = df.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            # 1. Label Encoding (1)
            le = LabelEncoder()
            features[f'{col}_label'] = le.fit_transform(df[col].fillna('Unknown'))

            # 2. One-Hot Encoding (é’ˆå¯¹ä½åŸºæ•°ï¼Œ< 10)
            if df[col].nunique() < 10:
                dummies = pd.get_dummies(df[col].fillna('Unknown'), prefix=f'{col}_onehot')
                features = pd.concat([features, dummies], axis=1)

            # 3. Binary Encoding (é’ˆå¯¹ä¸­åŸºæ•°ï¼Œ10-100)
            if 10 <= df[col].nunique() < 100:
                be = ce.BinaryEncoder(cols=[col])
                binary_encoded = be.fit_transform(df[[col]].fillna('Unknown'))
                features = pd.concat([features, binary_encoded], axis=1)

            # 4. BaseN Encoding (Base5)
            if df[col].nunique() < 50:
                bne = ce.BaseNEncoder(base=5, cols=[col])
                basen_encoded = bne.fit_transform(df[[col]].fillna('Unknown'))
                features = pd.concat([features, basen_encoded], axis=1)

            # 5. Ordinal Encoding (é’ˆå¯¹æœ‰åºç±»åˆ«)
            if self._is_ordinal(col):
                oe = ce.OrdinalEncoder(cols=[col])
                features[f'{col}_ordinal'] = oe.fit_transform(df[[col]].fillna('Unknown'))

        return features

    def _is_ordinal(self, col: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰åºç±»åˆ«"""
        ordinal_cols = ['targeting_age_range', 'video_length', 'ad_format']
        return any(ord_col in col for ord_col in ordinal_cols)
```

### 3.2 ç›®æ ‡ç¼–ç æŠ€æœ¯ï¼ˆ15+ï¼‰

```python
    def extract_target_encoding(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç›®æ ‡ç¼–ç æŠ€æœ¯ (15+)

        åŒ…æ‹¬:
        - Target Encoding (Mean Encoding)
        - Smoothed Target Encoding
        - Leave-One-Out Target Encoding
        - M-Estimate Encoding
        - WOE (Weight of Evidence) Encoding
        """

        features = pd.DataFrame(index=df.index)
        categorical_cols = df.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            # 1. Target Encoding (1)
            target_mean = df.groupby(col)['roas'].mean()
            features[f'{col}_target_enc'] = df[col].map(target_mean).fillna(df['roas'].mean())

            # 2. Smoothed Target Encoding (1)
            smoothing_factor = 10
            count = df[col].value_counts()
            global_mean = df['roas'].mean()
            smoothed_mean = (count * features[f'{col}_target_enc'] + smoothing_factor * global_mean) / (count + smoothing_factor)
            features[f'{col}_target_enc_smooth'] = df[col].map(smoothed_mean)

            # 3. Leave-One-Out Target Encoding (1)
            loo_mean = df.groupby(col)['roas'].transform(lambda x: (x.sum() - x) / (len(x) - 1))
            features[f'{col}_target_enc_loo'] = loo_mean.fillna(df['roas'].mean())

            # 4. M-Estimate Encoding (1)
            m = 100
            m_estimate = (count * features[f'{col}_target_enc'] + m * global_mean) / (count + m)
            features[f'{col}_m_estimate'] = df[col].map(m_estimate)

            # 5. WOE Encoding (é’ˆå¯¹äºŒåˆ†ç±»é—®é¢˜) (1)
            # è¿™é‡Œç®€åŒ–ä¸ºåŸºäº ROAS > threshold çš„äºŒåˆ†ç±»
            df['high_roas'] = (df['roas'] > df['roas'].median()).astype(int)
            woe = self._calculate_woe(df[col], df['high_roas'])
            features[f'{col}_woe'] = df[col].map(woe).fillna(0)

            # 6. Target Encoding with CV (1)
            # K-Fold ç›®æ ‡ç¼–ç é˜²æ­¢è¿‡æ‹Ÿåˆ
            features[f'{col}_target_enc_cv'] = self._kfold_target_encoding(df, col, 'roas', k=5)

        return features

    def _calculate_woe(self, categorical: pd.Series, target: pd.Series) -> dict:
        """è®¡ç®— Weight of Evidence"""
        woe_dict = {}
        for category in categorical.unique():
            cat_data = target[categorical == category]
            pos = cat_data.sum()
            neg = len(cat_data) - pos
            total_pos = target.sum()
            total_neg = len(target) - total_pos

            if pos == 0 or neg == 0:
                woe_dict[category] = 0
            else:
                woe_dict[category] = np.log((pos / total_pos) / (neg / total_neg))
        return woe_dict

    def _kfold_target_encoding(self, df: pd.DataFrame, cat_col: str, target_col: str, k: int = 5) -> pd.Series:
        """K-Fold ç›®æ ‡ç¼–ç """
        from sklearn.model_selection import KFold

        kf = KFold(n_splits=k, shuffle=True, random_state=42)
        encoded = pd.Series(index=df.index, dtype=float)

        for train_idx, val_idx in kf.split(df):
            train_mean = df.iloc[train_idx].groupby(cat_col)[target_col].mean()
            encoded.iloc[val_idx] = df.iloc[val_idx][cat_col].map(train_mean)

        return encoded.fillna(df[target_col].mean())
```

### 3.3 é¢‘ç‡å’Œè®¡æ•°ç¼–ç ï¼ˆ10+ï¼‰

```python
    def extract_frequency_encoding(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é¢‘ç‡å’Œè®¡æ•°ç¼–ç  (10+)

        åŒ…æ‹¬:
        - Count Encoding
        - Frequency Encoding
        - Target Count Encoding
        - Cumulative Count Encoding
        - Rare Category Encoding
        """

        features = pd.DataFrame(index=df.index)
        categorical_cols = df.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            # 1. Count Encoding (1)
            count = df[col].value_counts()
            features[f'{col}_count'] = df[col].map(count)

            # 2. Frequency Encoding (1)
            features[f'{col}_freq'] = df[col].map(count / len(df))

            # 3. Log Frequency (1)
            features[f'{col}_log_freq'] = np.log1p(features[f'{col}_count'])

            # 4. Target Count (1)
            target_count = df.groupby(col)['roas'].count()
            features[f'{col}_target_count'] = df[col].map(target_count)

            # 5. Cumulative Count (1)
            features[f'{col}_cumcount'] = df.groupby(col).cumcount() + 1

            # 6. Rare Category Indicator (1)
            rare_threshold = 0.01  # å‡ºç°é¢‘ç‡ < 1%
            features[f'{col}_is_rare'] = (features[f'{col}_freq'] < rare_threshold).astype(int)

            # 7. Category Frequency Rank (1)
            features[f'{col}_freq_rank'] = df[col].map(count.rank(ascending=False))

            # 8. Category Density (1)
            features[f'{col}_density'] = df[col].map(count / len(df))

        return features
```

### 3.4 åµŒå…¥å’Œç›¸ä¼¼åº¦ç¼–ç ï¼ˆ15+ï¼‰

```python
    def extract_embedding_encoding(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åµŒå…¥å’Œç›¸ä¼¼åº¦ç¼–ç  (15+)

        åŒ…æ‹¬:
        - Entity Embedding
        - TF-IDF (é’ˆå¯¹æ–‡æœ¬)
        - Similarity Encoding
        - Hashing Encoding
        - Polynomial Coding
        """

        features = pd.DataFrame(index=df.index)

        # 1. Hashing Encoder (é’ˆå¯¹é«˜åŸºæ•°) (3)
        high_card_cols = ['ad_id', 'adset_id', 'campaign_id']
        for col in high_card_cols:
            if col in df.columns:
                he = ce.HashingEncoder(cols=[col], n_components=8)
                hashed = he.fit_transform(df[[col]].fillna('Unknown'))
                features = pd.concat([features, hashed], axis=1)

        # 2. Text TF-IDF Features (é’ˆå¯¹å¹¿å‘Šæè¿°) (5)
        if 'ad_description' in df.columns:
            from sklearn.feature_extraction.text import TfidfVectorizer

            tfidf = TfidfVectorizer(max_features=20, ngram_range=(1, 2))
            tfidf_matrix = tfidf.fit_transform(df['ad_description'].fillna(''))

            tfidf_df = pd.DataFrame(
                tfidf_matrix.toarray(),
                columns=[f'desc_tfidf_{i}' for i in range(20)],
                index=df.index
            )
            features = pd.concat([features, tfidf_df], axis=1)

        # 3. Polynomial Coding (é’ˆå¯¹æœ‰åºç±»åˆ«) (2)
        ordinal_cols = [col for col in df.columns if self._is_ordinal(col)]
        for col in ordinal_cols:
            n_categories = df[col].nunique()
            for i in range(min(3, n_categories - 1)):
                features[f'{col}_poly_{i}'] = (pd.factorize(df[col].fillna('Unknown'))[0] ** i)

        return features
```

---

## ğŸ”„ Part 4: å…¨é¢çš„äº¤äº’ç‰¹å¾å·¥ç¨‹ï¼ˆ500+ ç»„åˆï¼‰

### 4.1 æ•°å€¼-æ•°å€¼äº¤äº’ï¼ˆ200+ï¼‰

```python
class ComprehensiveInteractionFeatures:
    """å…¨é¢çš„äº¤äº’ç‰¹å¾æå–"""

    def extract_numerical_interactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ•°å€¼-æ•°å€¼äº¤äº’ç‰¹å¾ (200+)

        åŒ…æ‹¬:
        - ç®—æœ¯äº¤äº’: +, -, *, /
        - å¤šé¡¹å¼äº¤äº’
        - æ¯”ç‡äº¤äº’
        - å¯¹æ•°äº¤äº’
        - æŒ‡æ•°äº¤äº’
        - åˆ†æ®µäº¤äº’
        """

        features = pd.DataFrame(index=df.index)
        numeric_cols = ['impressions', 'clicks', 'spend', 'conversions', 'revenue', 'roas', 'ctr', 'cvr']

        # 1. ç®—æœ¯äº¤äº’: åŠ æ³• (21) - nC2
        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i+1:]:
                features[f'{col1}_plus_{col2}'] = df[col1] + df[col2]

        # 2. ç®—æœ¯äº¤äº’: ä¹˜æ³• (21)
        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i+1:]:
                features[f'{col1}_times_{col2}'] = df[col1] * df[col2]

        # 3. ç®—æœ¯äº¤äº’: å‡æ³• (21)
        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i+1:]:
                features[f'{col1}_minus_{col2}'] = df[col1] - df[col2]
                features[f'{col2}_minus_{col1}'] = df[col2] - df[col1]

        # 4. ç®—æœ¯äº¤äº’: é™¤æ³• (42)
        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols:
                if col1 != col2:
                    features[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-6)

        # 5. å¤šé¡¹å¼äº¤äº’: äºŒæ¬¡ (7)
        for col in numeric_cols:
            features[f'{col}_squared'] = df[col] ** 2

        # 6. å¤šé¡¹å¼äº¤äº’: ä¸‰æ¬¡ (7)
        for col in numeric_cols:
            features[f'{col}_cubed'] = df[col] ** 3

        # 7. å¤šé¡¹å¼äº¤äº’: å¹³æ–¹æ ¹ (7)
        for col in numeric_cols:
            features[f'{col}_sqrt'] = np.sqrt(df[col].abs())

        # 8. å¯¹æ•°äº¤äº’ (14)
        for col1, col2 in [('impressions', 'clicks'), ('spend', 'conversions'), ('revenue', 'spend')]:
            features[f'{col1}_log_plus_{col2}_log'] = np.log1p(df[col1]) + np.log1p(df[col2])
            features[f'{col1}_log_times_{col2}_log'] = np.log1p(df[col1]) * np.log1p(df[col2])

        # 9. æŒ‡æ•°äº¤äº’ (7)
        for col in numeric_cols[:3]:  # åªå¯¹å‰3ä¸ªç‰¹å¾è®¡ç®—
            features[f'{col}_exp'] = np.exp(df[col] / (df[col].max() + 1e-6))  # å½’ä¸€åŒ–é˜²æ­¢æº¢å‡º

        # 10. åˆ†æ®µäº¤äº’ (14)
        for col1, col2 in [('impressions', 'spend'), ('clicks', 'conversions')]:
            # é«˜-é«˜ç»„åˆ
            features[f'{col1}_high_{col2}_high'] = ((df[col1] > df[col1].median()) & (df[col2] > df[col2].median())).astype(int)
            # é«˜-ä½ç»„åˆ
            features[f'{col1}_high_{col2}_low'] = ((df[col1] > df[col1].median()) & (df[col2] <= df[col2].median())).astype(int)
            # ä½-é«˜ç»„åˆ
            features[f'{col1}_low_{col2}_high'] = ((df[col1] <= df[col1].median()) & (df[col2] > df[col2].median())).astype(int)
            # ä½-ä½ç»„åˆ
            features[f'{col1}_low_{col2}_low'] = ((df[col1] <= df[col1].median()) & (df[col2] <= df[col2].median())).astype(int)

        return features
```

### 4.2 ç±»åˆ«-ç±»åˆ«äº¤äº’ï¼ˆ150+ï¼‰

```python
    def extract_categorical_interactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç±»åˆ«-ç±»åˆ«äº¤äº’ç‰¹å¾ (150+)

        åŒ…æ‹¬:
        - ç»„åˆç‰¹å¾
        - äº¤å‰ç»Ÿè®¡
        - æ¡ä»¶æ¦‚ç‡
        - å…±ç°æ¨¡å¼
        """

        features = pd.DataFrame(index=df.index)
        categorical_cols = ['campaign_objective', 'ad_format', 'targeting_gender', 'targeting_age_range', 'call_to_action']

        # 1. ä¸¤ä¸¤ç»„åˆ (10) - nC2 for 5 cols
        for i, col1 in enumerate(categorical_cols):
            for col2 in categorical_cols[i+1:]:
                features[f'{col1}_{col2}_combo'] = df[col1].astype(str) + '_' + df[col2].astype(str)

        # 2. ä¸‰ä¸‰ç»„åˆ (10) - nC3
        import itertools
        for col1, col2, col3 in itertools.combinations(categorical_cols, 3):
            features[f'{col1}_{col2}_{col3}_combo'] = df[col1].astype(str) + '_' + df[col2].astype(str) + '_' + df[col3].astype(str)

        # 3. äº¤äº’ç»Ÿè®¡ (é’ˆå¯¹ç»„åˆç‰¹å¾) (50)
        combo_features = [col for col in features.columns if '_combo' in col]
        for combo in combo_features[:5]:  # åªå¯¹å‰5ä¸ªç»„åˆè®¡ç®—
            # ç»„åˆçš„é¢‘ç‡
            combo_freq = features[combo].value_counts(normalize=True)
            features[f'{combo}_freq'] = features[combo].map(combo_freq)

            # ç»„åˆçš„ç›®æ ‡å‡å€¼
            if 'roas' in df.columns:
                combo_target_mean = df.groupby(features[combo])['roas'].mean()
                features[f'{combo}_target_mean'] = features[combo].map(combo_target_mean)

        # 4. æ¡ä»¶æ¦‚ç‡ (30)
        for col1, col2 in itertools.combinations(categorical_cols[:4], 2):  # 4ä¸ªç±»åˆ«å–2ä¸ª
            # P(col2 | col1)
            conditional_prob = df.groupby(col1)[col2].value_counts(normalize=True)
            features[f'{col2}_given_{col1}_prob'] = df.apply(
                lambda row: conditional_prob.get((row[col1], row[col2]), 0),
                axis=1
            )

        # 5. å…±ç°æŒ‡æ ‡ (20)
        # è®¡ç®—ä¸¤ä¸ªç±»åˆ«ç‰¹å¾åŒæ—¶å‡ºç°çš„å¼ºåº¦
        for col1, col2 in itertools.combinations(categorical_cols[:5], 2):
            # Pointwise Mutual Information
            p_col1 = df[col1].value_counts(normalize=True)
            p_col2 = df[col2].value_counts(normalize=True)
            p_col1_col2 = df.groupby([col1, col2]).size() / len(df)

            pmi = []
            for _, row in df.iterrows():
                p_xy = p_col1_col2.get((row[col1], row[col2]), 1e-10)
                p_x = p_col1.get(row[col1], 1e-10)
                p_y = p_col2.get(row[col2], 1e-10)
                pmi_val = np.log2(p_xy / (p_x * p_y + 1e-10))
                pmi.append(pmi_val)

            features[f'{col1}_{col2}_pmi'] = pmi

        return features
```

### 4.3 æ•°å€¼-ç±»åˆ«äº¤äº’ï¼ˆ100+ï¼‰

```python
    def extract_numerical_categorical_interactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ•°å€¼-ç±»åˆ«äº¤äº’ç‰¹å¾ (100+)

        åŒ…æ‹¬:
        - æŒ‰ç±»åˆ«åˆ†ç»„çš„æ•°å€¼ç»Ÿè®¡
        - ç±»åˆ«æ¡ä»¶ä¸‹çš„æ•°å€¼ç‰¹å¾
        - æ•°å€¼-ç±»åˆ«ç»„åˆç¼–ç 
        """

        features = pd.DataFrame(index=df.index)
        numeric_cols = ['impressions', 'clicks', 'spend', 'roas']
        categorical_cols = ['campaign_objective', 'ad_format', 'targeting_gender']

        # 1. æŒ‰ç±»åˆ«åˆ†ç»„çš„æ•°å€¼ç»Ÿè®¡ (36) - 3 cat * 4 num * 3 stats
        for cat_col in categorical_cols:
            if cat_col not in df.columns:
                continue
            for num_col in numeric_cols:
                if num_col not in df.columns:
                    continue

                # æ¯ä¸ªç±»åˆ«çš„å‡å€¼
                group_mean = df.groupby(cat_col)[num_col].transform('mean')
                features[f'{num_col}_mean_by_{cat_col}'] = group_mean

                # æ¯ä¸ªç±»åˆ«çš„æ ‡å‡†å·®
                group_std = df.groupby(cat_col)[num_col].transform('std')
                features[f'{num_col}_std_by_{cat_col}'] = group_std

                # æ¯ä¸ªç±»åˆ«çš„æ’å
                group_rank = df.groupby(cat_col)[num_col].rank(pct=True)
                features[f'{num_col}_rank_in_{cat_col}'] = group_rank

        # 2. æ•°å€¼ä¸ç±»åˆ«çš„åå·® (24)
        for cat_col in categorical_cols:
            if cat_col not in df.columns:
                continue
            for num_col in numeric_cols:
                if num_col not in df.columns:
                    continue

                group_mean = df.groupby(cat_col)[num_col].transform('mean')
                features[f'{num_col}_deviation_from_{cat_col}_mean'] = df[num_col] - group_mean
                features[f'{num_col}_ratio_to_{cat_col}_mean'] = df[num_col] / (group_mean + 1e-6)

        # 3. ç±»åˆ«æ¡ä»¶ä¸‹çš„æ•°å€¼ç‰¹å¾ (30)
        for cat_col in categorical_cols:
            if cat_col not in df.columns:
                continue
            for num_col in numeric_cols:
                if num_col not in df.columns:
                    continue

                # ç±»åˆ« one-hot åä¸æ•°å€¼çš„ä¹˜ç§¯
                for category in df[cat_col].unique()[:3]:  # é™åˆ¶ç±»åˆ«æ•°
                    category_encoded = (df[cat_col] == category).astype(int)
                    features[f'{num_col}_for_{cat_col}_{category}'] = df[num_col] * category_encoded

        # 4. æ•°å€¼-ç±»åˆ«ç»„åˆç›®æ ‡ç¼–ç  (12)
        if 'roas' in df.columns:
            for cat_col in categorical_cols[:2]:  # åªå¯¹å‰2ä¸ªç±»åˆ«
                if cat_col not in df.columns:
                    continue
                for num_col in numeric_cols[:2]:  # åªå¯¹å‰2ä¸ªæ•°å€¼
                    if num_col not in df.columns:
                        continue

                    # æ•°å€¼åˆ†æ¡¶ + ç±»åˆ«ç»„åˆçš„ç›®æ ‡ç¼–ç 
                    df[f'{num_col}_binned'] = pd.cut(df[num_col], bins=5, labels=False)
                    combo_target_mean = df.groupby([cat_col, f'{num_col}_binned'])['roas'].mean()
                    features[f'{cat_col}_{num_col}_combo_target'] = df.apply(
                        lambda row: combo_target_mean.get((row[cat_col], row[f'{num_col}_binned']), df['roas'].mean()),
                        axis=1
                    )

        return features
```

### 4.4 æ—¶é—´-äº¤äº’ç‰¹å¾ï¼ˆ50+ï¼‰

```python
    def extract_temporal_interactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ—¶é—´-äº¤äº’ç‰¹å¾ (50+)

        åŒ…æ‹¬:
        - æ—¶é—´çª—å£å†…çš„äº¤äº’
        - æ—¶é—´-ç±»åˆ«ç»„åˆ
        - æ—¶é—´è¶‹åŠ¿-ç‰¹å¾äº¤äº’
        """

        features = pd.DataFrame(index=df.index)
        df['date'] = pd.to_datetime(df['date'])

        # 1. æ—¶é—´-ç‰¹å¾äº¤äº’ (20)
        numeric_cols = ['impressions', 'spend', 'roas']
        for col in numeric_cols:
            if col not in df.columns:
                continue

            # å·¥ä½œæ—¥ * ç‰¹å¾
            features[f'{col}_weekday'] = df[col] * df['date'].dt.weekday

            # æœˆåˆ * ç‰¹å¾
            features[f'{col}_month_start'] = df[col] * (df['date'].dt.day <= 7).astype(int)

            # å‘¨æœ« * ç‰¹å¾
            features[f'{col}_weekend'] = df[col] * (df['date'].dt.weekday >= 5).astype(int)

        # 2. æ—¶é—´-ç±»åˆ«äº¤äº’ (15)
        categorical_cols = ['campaign_objective', 'ad_format']
        for cat_col in categorical_cols:
            if cat_col not in df.columns:
                continue

            # ç±»åˆ« * å°æ—¶
            df['hour'] = df['date'].dt.hour
            for category in df[cat_col].unique()[:3]:
                features[f'{cat_col}_{category}_hour'] = ((df[cat_col] == category) * df['hour']).astype(int)

        # 3. è¶‹åŠ¿-ç‰¹å¾äº¤äº’ (10)
        for col in ['spend', 'roas']:
            if col not in df.columns:
                continue

            # è¶‹åŠ¿ * å½“å‰å€¼
            trend = df[col].rolling(7).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
            features[f'{col}_trend_interaction'] = df[col] * trend

            # æ³¢åŠ¨ç‡ * å½“å‰å€¼
            volatility = df[col].rolling(7).std()
            features[f'{col}_volatility_interaction'] = df[col] * volatility

        # 4. ç´¯ç§¯-ç‰¹å¾äº¤äº’ (5)
        features['cumulative_spend_roas'] = df['spend'].cumsum() * df['roas']
        features['cumulative_impressions_ctr'] = df['impressions'].cumsum() * df['ctr']

        return features
```

---

## ğŸ“ˆ Part 5: å…¨é¢çš„æ—¶é—´åºåˆ—ç‰¹å¾å·¥ç¨‹ï¼ˆ150+ ç‰¹å¾ï¼‰

### 5.1 é«˜çº§æ—¶é—´åºåˆ—ç‰¹å¾

```python
class AdvancedTimeSeriesFeatures:
    """é«˜çº§æ—¶é—´åºåˆ—ç‰¹å¾"""

    def extract_advanced_timeseries_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é«˜çº§æ—¶é—´åºåˆ—ç‰¹å¾ (100+)

        åŒ…æ‹¬:
        - æ—¶é—´åºåˆ—åˆ†è§£
        - å‚…é‡Œå¶å˜æ¢
        - å°æ³¢å˜æ¢
        - åŠ¨æ€æ—¶é—´è§„æ•´
        - æ—¶é—´åºåˆ—å½¢çŠ¶ç‰¹å¾
        """

        features = pd.DataFrame(index=df.index)
        df = df.sort_values('date')

        for col in ['spend', 'roas', 'impressions']:
            if col not in df.columns:
                continue

            # 1. æ—¶é—´åºåˆ—åˆ†è§£ (15)
            if len(df) >= 14:  # éœ€è¦è¶³å¤Ÿæ•°æ®
                try:
                    decomposition = seasonal_decompose(df[col].fillna(0), model='additive', period=7)
                    features[f'{col}_trend'] = decomposition.trend
                    features[f'{col}_seasonal'] = decomposition.seasonal
                    features[f'{col}_residual'] = decomposition.resid

                    # è¶‹åŠ¿å¼ºåº¦
                    features[f'{col}_trend_strength'] = np.abs(decomposition.trend).rolling(7).mean()

                    # å­£èŠ‚æ€§å¼ºåº¦
                    features[f'{col}_seasonal_strength'] = np.abs(decomposition.seasonal).rolling(7).mean()

                    # æ®‹å·®æ³¢åŠ¨
                    features[f'{col}_residual_volatility'] = decomposition.resid.rolling(7).std()
                except:
                    pass

            # 2. å¹³ç¨³æ€§æ£€éªŒ (3)
            if len(df) >= 30:
                try:
                    from statsmodels.tsa.stattools import adfuller, kpss
                    result_adf = adfuller(df[col].dropna())
                    features[f'{col}_adf_statistic'] = result_adf[0]
                    features[f'{col}_adf_pvalue'] = result_adf[1]
                    features[f'{col}_is_stationary'] = (result_adf[1] < 0.05).astype(int)
                except:
                    pass

            # 3. è‡ªç›¸å…³å’Œåè‡ªç›¸å…³ (10)
            for lag in [1, 2, 3, 7, 14]:
                features[f'{col}_autocorr_lag{lag}'] = df[col].autocorr(lag=lag)

                # Partial autocorrelation
                try:
                    from statsmodels.tsa.stattools import pacf
                    pacf_values = pacf(df[col].dropna(), nlags=lag)
                    features[f'{col}_pacf_lag{lag}'] = pacf_values[lag] if lag < len(pacf_values) else 0
                except:
                    features[f'{col}_pacf_lag{lag}'] = 0

            # 4. å˜åŒ–ç‚¹æ£€æµ‹ (5)
            features[f'{col}_change_score'] = self._detect_change_points(df[col])
            features[f'{col}_cusum'] = self._cusum_statistic(df[col])
            features[f'{col}_zscore_change'] = np.abs((df[col] - df[col].rolling(30).mean()) / df[col].rolling(30).std())
            features[f'{col}_mean_diff_short_long'] = df[col].rolling(7).mean() - df[col].rolling(30).mean()
            features[f'{col}_ratio_short_long'] = df[col].rolling(7).mean() / (df[col].rolling(30).mean() + 1e-6)

            # 5. æ—¶é—´åºåˆ—å½¢çŠ¶ç‰¹å¾ (8)
            features[f'{col}_curve_length'] = self._curve_length(df[col])
            features[f'{col}_zero_crossing_rate'] = self._zero_crossing_rate(df[col])
            features[f'{col}_peak_count'] = self._peak_count(df[col])
            features[f'{col}_trough_count'] = self._trough_count(df[col])
            features[f'{col}_slope_sign_changes'] = self._slope_sign_changes(df[col])
            features[f'{col}_local_maxima'] = df[col].rolling(5, center=True).apply(lambda x: x[2] == max(x))
            features[f'{col}_local_minima'] = df[col].rolling(5, center=True).apply(lambda x: x[2] == min(x))
            features[f'{col}_monotonicity'] = self._monotonicity(df[col])

        return features

    def _detect_change_points(self, series: pd.Series, window: int = 7) -> pd.Series:
        """æ£€æµ‹å˜åŒ–ç‚¹"""
        mean_short = series.rolling(window).mean()
        mean_long = series.rolling(window * 4).mean()
        return np.abs((mean_short - mean_long) / (mean_long + 1e-6))

    def _cusum_statistic(self, series: pd.Series) -> pd.Series:
        """CUSUM ç»Ÿè®¡é‡"""
        target = series.rolling(30).mean()
        return (series - target).cumsum()

    def _curve_length(self, series: pd.Series) -> float:
        """æ›²çº¿é•¿åº¦"""
        diff = series.diff().fillna(0)
        return np.sqrt(1 + diff**2).sum()

    def _zero_crossing_rate(self, series: pd.Series) -> float:
        """é›¶äº¤å‰ç‡"""
        centered = series - series.mean()
        return (centered.diff().fillna(0) < 0).astype(int).sum() / len(series)

    def _peak_count(self, series: pd.Series) -> int:
        """å³°å€¼æ•°é‡"""
        from scipy.signal import find_peaks
        peaks, _ = find_peaks(series.dropna().values)
        return len(peaks)

    def _trough_count(self, series: pd.Series) -> int:
        """è°·å€¼æ•°é‡"""
        from scipy.signal import find_peaks
        troughs, _ = find_peaks(-series.dropna().values)
        return len(troughs)

    def _slope_sign_changes(self, series: pd.Series) -> int:
        """æ–œç‡ç¬¦å·å˜åŒ–æ¬¡æ•°"""
        diff = series.diff().fillna(0)
        sign_changes = ((diff > 0) != (diff.shift(1) > 0)).astype(int)
        return sign_changes.sum()

    def _monotonicity(self, series: pd.Series) -> float:
        """å•è°ƒæ€§"""
        from scipy.stats import pearsonr
        x = np.arange(len(series))
        correlation, _ = pearsonr(x, series.fillna(0))
        return abs(correlation)
```

### 5.2 é¢‘åŸŸç‰¹å¾

```python
    def extract_frequency_domain_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é¢‘åŸŸç‰¹å¾ (30+)

        åŒ…æ‹¬:
        - FFT ç‰¹å¾
        - åŠŸç‡è°±å¯†åº¦
        - é¢‘è°±ç†µ
        - ä¸»é¢‘ç‡
        """

        features = pd.DataFrame(index=df.index)

        for col in ['spend', 'roas', 'impressions']:
            if col not in df.columns:
                continue

            # 1. FFT å˜æ¢ (8)
            fft_values = np.fft.fft(df[col].fillna(0).values)
            fft_freq = np.fft.fftfreq(len(df))

            # ä¸»é¢‘ç‡
            dominant_freq_idx = np.argmax(np.abs(fft_values[1:len(fft_values)//2])) + 1
            features[f'{col}_dominant_freq'] = np.abs(fft_freq[dominant_freq_idx])
            features[f'{col}_dominant_freq_power'] = np.abs(fft_values[dominant_freq_idx])

            # é¢‘è°±èƒ½é‡
            features[f'{col}_spectral_energy'] = np.sum(np.abs(fft_values)**2)

            # é¢‘è°±è´¨å¿ƒ
            power_spectrum = np.abs(fft_values)**2
            features[f'{col}_spectral_centroid'] = np.sum(fft_freq * power_spectrum) / (np.sum(power_spectrum) + 1e-6)

            # é¢‘è°±å¸¦å®½
            features[f'{col}_spectral_bandwidth'] = np.sqrt(
                np.sum(((fft_freq - features[f'{col}_spectral_centroid'])**2) * power_spectrum) /
                (np.sum(power_spectrum) + 1e-6)
            )

            # é¢‘è°±ç†µ
            power_spectrum_norm = power_spectrum / (np.sum(power_spectrum) + 1e-6)
            features[f'{col}_spectral_entropy'] = -np.sum(power_spectrum_norm * np.log2(power_spectrum_norm + 1e-6))

            # ä½é¢‘èƒ½é‡æ¯”ä¾‹
            low_freq_mask = np.abs(fft_freq) < 0.1
            features[f'{col}_low_freq_energy_ratio'] = (
                np.sum(power_spectrum[low_freq_mask]) / (np.sum(power_spectrum) + 1e-6)
            )

            # é«˜é¢‘èƒ½é‡æ¯”ä¾‹
            high_freq_mask = np.abs(fft_freq) > 0.3
            features[f'{col}_high_freq_energy_ratio'] = (
                np.sum(power_spectrum[high_freq_mask]) / (np.sum(power_spectrum) + 1e-6)
            )

        return features
```

### 5.3 æ—¶é—´åºåˆ—æ¨¡å¼ç‰¹å¾

```python
    def extract_pattern_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ—¶é—´åºåˆ—æ¨¡å¼ç‰¹å¾ (20+)

        åŒ…æ‹¬:
        - å‘¨æœŸæ€§æ¨¡å¼
        - è¶‹åŠ¿æ¨¡å¼
        - å­£èŠ‚æ€§æ¨¡å¼
        """

        features = pd.DataFrame(index=df.index)
        df = df.sort_values('date')

        for col in ['spend', 'roas']:
            if col not in df.columns:
                continue

            # 1. å‘¨æœŸæ€§ç‰¹å¾ (7)
            # å‘¨å†…å‘¨æœŸæ€§
            df['day_of_week'] = df['date'].dt.dayofweek
            dow_pattern = df.groupby('day_of_week')[col].mean()
            features[f'{col}_dow_pattern_strength'] = dow_pattern.std() / (dow_pattern.mean() + 1e-6)

            # æœˆå†…å‘¨æœŸæ€§
            df['day_of_month'] = df['date'].dt.day
            dom_pattern = df.groupby('day_of_month')[col].mean()
            features[f'{col}_dom_pattern_strength'] = dom_pattern.std() / (dom_pattern.mean() + 1e-6)

            # 2. è¶‹åŠ¿æ¨¡å¼ (5)
            # çº¿æ€§è¶‹åŠ¿å¼ºåº¦
            x = np.arange(len(df))
            slope, intercept = np.polyfit(x, df[col].fillna(0), 1)
            y_pred = slope * x + intercept
            ss_res = np.sum((df[col].fillna(0) - y_pred) ** 2)
            ss_tot = np.sum((df[col].fillna(0) - df[col].mean()) ** 2)
            features[f'{col}_trend_r2'] = 1 - (ss_res / (ss_tot + 1e-6))
            features[f'{col}_trend_slope'] = slope
            features[f'{col}_trend_direction'] = (slope > 0).astype(int)
            features[f'{col}_is_uptrend'] = (slope > 0) & (features[f'{col}_trend_r2'] > 0.5)
            features[f'{col}_is_downtrend'] = (slope < 0) & (features[f'{col}_trend_r2'] > 0.5)

            # 3. å­£èŠ‚æ€§æ¨¡å¼ (5)
            # å­£èŠ‚æ€§å¼ºåº¦
            detrended = df[col] - df[col].rolling(7).mean()
            features[f'{col}_seasonality_strength'] = detrended.std() / (df[col].std() + 1e-6)

            # å‘¨æœŸæ£€æµ‹
            from scipy.signal import find_peaks
            autocorr = [df[col].autocorr(lag=lag) for lag in range(1, 31)]
            peaks, _ = find_peaks(autocorr)
            features[f'{col}_dominant_period'] = peaks[0] if len(peaks) > 0 else 0
            features[f'{col}_has_weekly_seasonality'] = (7 in peaks)
            features[f'{col}_has_monthly_seasonality'] = (30 in peaks)

        return features
```

---

## ğŸ“Š Part 6: ç‰¹å¾é€‰æ‹©å’Œé™ç»´

### 6.1 ç‰¹å¾é€‰æ‹©æ–¹æ³•

```python
class FeatureSelector:
    """ç‰¹å¾é€‰æ‹©å™¨"""

    def select_features(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        method: str = 'combined',
        n_features: int = 100
    ) -> list:
        """
        ç‰¹å¾é€‰æ‹©

        æ–¹æ³•:
        - variance: æ–¹å·®é˜ˆå€¼
        - correlation: ç›¸å…³æ€§è¿‡æ»¤
        - mutual_info: äº’ä¿¡æ¯
        - chi2: å¡æ–¹æ£€éªŒ
        - importance: æ¨¡å‹ç‰¹å¾é‡è¦æ€§
        - combined: ç»„åˆæ–¹æ³•
        """

        selected_features = []

        if method in ['variance', 'combined']:
            # 1. æ–¹å·®é˜ˆå€¼
            variance_selector = VarianceThreshold(threshold=0.01)
            variance_selector.fit(X)
            selected_features.append(X.columns[variance_selector.get_support()])

        if method in ['correlation', 'combined']:
            # 2. ç›¸å…³æ€§è¿‡æ»¤
            corr_matrix = X.corr().abs()
            upper_triangle = corr_matrix.where(
                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
            )
            high_corr_features = [
                column for column in upper_triangle.columns
                if any(upper_triangle[column] > 0.95)
            ]
            selected_features.append([col for col in X.columns if col not in high_corr_features])

        if method in ['mutual_info', 'combined']:
            # 3. äº’ä¿¡æ¯
            from sklearn.feature_selection import mutual_info_regression
            mi_scores = mutual_info_regression(X, y)
            mi_selected = X.columns[np.argsort(mi_scores)[-n_features:]]
            selected_features.append(mi_selected)

        if method == 'combined':
            # å–äº¤é›†
            selected = set(selected_features[0])
            for s in selected_features[1:]:
                selected.intersection_update(s)
            return list(selected)

        return selected_features[0]
```

### 6.2 é™ç»´æŠ€æœ¯

```python
class DimensionalityReducer:
    """é™ç»´å™¨"""

    def reduce_dimensions(
        self,
        X: pd.DataFrame,
        method: str = 'pca',
        n_components: int = 50
    ) -> pd.DataFrame:
        """
        é™ç»´

        æ–¹æ³•:
        - pca: ä¸»æˆåˆ†åˆ†æ
        - tsne: t-SNE
        - umap: UMAP
        - autoencoder: è‡ªç¼–ç å™¨
        """

        if method == 'pca':
            from sklearn.decomposition import PCA
            reducer = PCA(n_components=n_components)
            reduced = reducer.fit_transform(X)

            columns = [f'pc_{i}' for i in range(n_components)]
            return pd.DataFrame(reduced, columns=columns, index=X.index)

        elif method == 'tsne':
            from sklearn.manifold import TSNE
            reducer = TSNE(n_components=n_components, random_state=42)
            reduced = reducer.fit_transform(X)

            columns = [f'tsne_{i}' for i in range(n_components)]
            return pd.DataFrame(reduced, columns=columns, index=X.index)

        elif method == 'umap':
            from umap import UMAP
            reducer = UMAP(n_components=n_components, random_state=42)
            reduced = reducer.fit_transform(X)

            columns = [f'umap_{i}' for i in range(n_components)]
            return pd.DataFrame(reduced, columns=columns, index=X.index)

        return X
```

---

## ğŸ“‹ é™„å½•

### A. ç‰¹å¾åç§°æ··æ·†å®Œæ•´æ˜ å°„ç¤ºä¾‹

```python
# å†…éƒ¨æ˜ å°„è¡¨ (config/feature_mappings/v1.0.json)
{
  "æ•°å€¼ç‰¹å¾": {
    "f1": "impressions_mean",
    "f2": "clicks_sum",
    "f3": "spend_median",
    "f4": "roas_std",
    "f5": "ctr",
    "f6": "cvr",
    "f7": "cpa",
    "f8": "cpc",
    "f9": "cpm",
    "f10": "roas_7d_avg",
    ...
  },

  "ç±»åˆ«ç‰¹å¾": {
    "f251": "campaign_objective_encoding",
    "f252": "ad_format_onehot_video",
    "f253": "targeting_gender_onehot_female",
    "f254": "call_to_action_encoding",
    ...
  },

  "æ—¶åºç‰¹å¾": {
    "f351": "impressions_rolling_mean_7d",
    "f352": "spend_lag_7d",
    "f353": "roas_trend_slope",
    "f354": "clicks_autocorr_lag7",
    ...
  },

  "äº¤äº’ç‰¹å¾": {
    "f401": "impressions_plus_clicks",
    "f402": "spend_times_roas",
    "f403": "objective_format_combo",
    "f404": "impressions_mean_by_objective",
    ...
  }
}
```

### B. ç‰¹å¾ç»Ÿè®¡æ€»è§ˆ

| ç‰¹å¾ç±»åˆ« | Phase 1 (Python) | Phase 2 (Spark) | Phase 3 (Streaming) |
|---------|------------------|-----------------|-------------------|
| **åŸºç¡€æ•°å€¼** | 50 | 50 | 50 |
| **é«˜çº§æ•°å€¼** | 60 | 80 | 100 |
| **åŸºç¡€ç±»åˆ«** | 20 | 30 | 40 |
| **é«˜çº§ç±»åˆ«** | 30 | 50 | 60 |
| **æ—¶åºåŸºç¡€** | 40 | 60 | 80 |
| **æ—¶åºé«˜çº§** | 30 | 50 | 70 |
| **äº¤äº’ç‰¹å¾** | 100 | 200 | 500 |
| **æ€»è®¡** | **330** | **520** | **900+** |

### C. æŠ€æœ¯æ ˆæ€»ç»“

| Phase | æ‰¹å¤„ç† | æµå¤„ç† | å­˜å‚¨ | è°ƒåº¦ |
|-------|--------|--------|------|------|
| **Phase 1** | Pandas/NumPy | - | CSV/Parquet | Cron |
| **Phase 2** | PySpark | - | S3 + Parquet | Airflow |
| **Phase 3** | PySpark | Lambda/Step Functions | Redis + DynamoDB | EventBridge |

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0
**æœ€åæ›´æ–°**: 2025-01-29
**ç»´æŠ¤è€…**: Data Engineering Team
**ç‰¹å¾æ€»æ•°**: 900+
